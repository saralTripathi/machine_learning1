{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uQ5qjinAjkYV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M_bKc4Cnjr6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a parameter?\n",
        "   \n",
        "    -  In the context of computer programming, a parameter is a variable used in a function or subroutine definition. It acts as a placeholder for values that will be passed into the function when it is called. These values are known as arguments\n",
        "\n",
        "2. What is correlation?\n",
        "\n",
        "   What does negative correlation mean?\n",
        "\n",
        "    -  Correlation is a statistical measure that describes the extent to which two variables change together. It indicates the strength and direction of the linear relationship between two variables.\n",
        "    - Negative correlation means that as one variable increases, the other variable tends to decrease. The value of the correlation coefficient for a negative correlation ranges from -1 to 0. A value of -1 indicates a perfect negative linear relationship, while a value closer to 0 indicates a weaker negative linear relationship.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "    - Machine Learning is a subfield of artificial intelligence that focuses on the development of algorithms that allow computers to learn from data without being explicitly programmed. The goal is to enable computers to identify patterns, make predictions, and improve their performance on tasks through experience.\n",
        "\n",
        "    - Main components OF machine learning is : Data,Algorithm,Model,Evaluation,Optimization\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not?\n",
        "    -  The loss value is a key metric in machine learning that helps determine how well a model is performing.\n",
        "    - The loss measures the difference between the model's predictions and the actual target values (ground truth). It quantifies how wrong the model is on a single data point or a batch.\n",
        "    - Lower loss = better predictions\n",
        "    - Higher loss = worse predictions\n",
        "    - As training progresses, a decreasing loss typically indicates that the model is learning and improving.\n",
        "    - If the loss stays high or fluctuates a lot, it might mean the model is not learning properly.\n",
        "    - If training loss is low but validation loss is high, your model is likely overfitting (memorizing training data but not generalizing well).\n",
        "    - If both training and validation losses are high, the model is underfitting (not learning enough).\n",
        "    - Loss ≠ Accuracy: A model can have a low loss but not necessarily high accuracy, especially in classification tasks. Loss provides more fine-grained feedback than accuracy.\n",
        "5. What are continuous and categorical variables?\n",
        "  - Continuous Variables : These are variables that can take on any value within a given range. They represent measurements and can be infinitely divided into smaller increments. Examples include:\n",
        "    - Height\n",
        "    - Weight\n",
        "    - Temperature\n",
        "    - TIme\n",
        "    - Sales revenue\n",
        "  - Categorical Variables: These are variables that can only take on a limited number of distinct values or categories. These values do not have a natural order or numerical meaning, although they might be represented by numbers in a dataset. Examples include:\n",
        "     - Color\n",
        "     - Gender\n",
        "     - Country of origin\n",
        "     - type of animal\n",
        "     - yes/no resposes\n",
        "\n",
        "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
        "   - When working with machine learning models, categorical variables need to be converted into a numerical format because most models cannot directly process string values . This process is called encoding\n",
        "   - Here are some common techniques for handling categorical variables:\n",
        "       - one hot encoding : This technique creates a new binary column for each unique category in the variable.\n",
        "       - label encoding : This method assigns a unique integer to each category\n",
        "       - binary encoding : This technique converts each integer label from Label Encoding into a binary code.\n",
        "       - frequency encoding : This method replaces each category with the frequency or count of its occurrences in the dataset .\n",
        "       - ordinal encoding : Similar to Label Encoding, this assigns integer values, but specifically for ordinal data, preserving the rank order of the categories.\n",
        "7. What do you mean by training and testing a dataset?\n",
        "   - Training Dataset: This is the subset of data used to train the machine learning model . During the training phase, the model learns patterns and relationships from this data by adjusting its internal parameters . The goal is for the model to learn how to make accurate predictions or perform a specific task based on the training data.\n",
        "   - Testing Dataset: This is a separate subset of data that is not used during the training process. Once the model is trained, the testing dataset is used to evaluate its performance on unseen data . By evaluating the model on data it has not seen before, we can assess how well it generalizes to new examples and get an unbiased estimate of its performance.\n",
        "\n",
        "8. What is sklearn.preprocessing?\n",
        "   - sklearn.preprocessing is a module in the scikit-learn library that provides a variety of functions for data preprocessing . Data preprocessing is a crucial step in the machine learning workflow, involving transforming raw data into a suitable format for model training.\n",
        "   - The sklearn.preprocessing module includes tools for tasks such as:\n",
        "        - Scaling features: Scaling techniques like MinMaxScaler and StandardScaler are used to transform features to a common range or distribution . This can help improve the performance of many machine learning algorithms.\n",
        "        - Encoding categorical variables: As mentioned earlier, this module provides methods like OneHotEncoder and LabelEncoder to convert categorical data into a numerical format that machine learning models can understand .\n",
        "        - Discretization: Converting continuous features into discrete bins.\n",
        "        - Polynomial features: Generating polynomial combinations of features.\n",
        "\n",
        "9. What is a Test set?\n",
        "   - A test set, also known as a testing dataset, is a separate subset of data that is not used during the training process of a machine learning model . Its primary purpose is to evaluate the performance of the trained model on unseen data .By using a test set, you can:\n",
        "        - Assess generaliztion\n",
        "        - Obtain an unbiased performance estimate\n",
        "        - identify overfitting\n",
        "\n",
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        "    \n",
        "    How do you approach a Machine Learning problem?\n",
        "    - We typically use train_test_split from scikit-learn.\n",
        "      test_size=0.2 means that 20% of the data will be allocated to the testing set, and the remaining 80% will be used for\n",
        "\n",
        "    - Clearly define the problem you want to solve, the goal, and the expected outcome.\n",
        "    - Gather the relevant data for your problem.\n",
        "    - Handle missing values, outliers, and transform the data into a suitable format for machine learning algorithms.\n",
        "    - Handle missing values, outliers, and transform the data into a suitable format for machine learning algorithms.\n",
        "    - Create new features from existing ones that might improve the model's performance.\n",
        "    - Split the data into training and testing sets (and sometimes a validation set) to train and evaluate your model.\n",
        "    - Train the selected model on the training data.\n",
        "    - Evaluate the trained model's performance on the testing data using appropriate metrics (e.g., accuracy, precision, recall, MSE, R-squared).\n",
        "    - Fine-tune the model's hyperparameters to improve performance.\n",
        "    - Use the trained and evaluated model to make predictions on new, unseen data or deploy it for practical use.\n",
        "    - Continuously monitor the model's performance and retrain it as needed with new data to maintain its accuracy.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data?  \n",
        "    - Performing EDA (Exploratory Data Analysis) before fitting a model is critical because it helps you understand your data deeply and avoid mistakes. Here's why EDA is necessary:\n",
        "\n",
        "       - You learn about:Data types (numerical, categorical, text, etc.),Number of features and samples,Target variable characteristics\n",
        "\n",
        "       - EDA helps identify:\n",
        "\n",
        "        Missing values\n",
        "\n",
        "        Duplicates\n",
        "\n",
        "       Outliers or invalid entries\n",
        "        - Check distributions of numerical features using histograms, boxplots, etc.\n",
        "        - Skewed data might require normalization or transformation.\n",
        "\n",
        "        - Understand Target Variable:is it imblanced or balanced\n",
        "        - Imbalanced data needs special handling (e.g., SMOTE, class weights).\n",
        "        - If your target is skewed, accuracy is not enough → use F1-score, precision/recall.\n",
        "\n",
        "        -  For regression with outliers → avoid RMSE, maybe use MAE.\n",
        "\n",
        "        - EDA prevents you from blindly fitting a model. It improves model quality, reduces errors, and often leads to valuable feature engineering and better decisions.\n",
        "12. What is correlation?\n",
        "    - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "    - It tells you how closely two variables move together.\n",
        "    - Correlation values range between -1 and 1:\n",
        "        - +1 → perfect positive correlation: both variables increase together.\n",
        "        - 0 → no correlation: variables are unrelated.\n",
        "        - -1 → perfect negative correlation: one increases, the other decreases.\n",
        "    - Helps in feature selection.\n",
        "    - Detects multicollinearity (high correlation between features).\n",
        "    - Avoids redundant data in models like linear regression.\n",
        "13. What does negative correlation mean?\n",
        "    - A negative correlation means that as one variable increases, the other decreases.\n",
        "    - A negative correlation has r < 0,where r is correlation coeficient\n",
        "    - A scatter plot of negatively correlated variables will show a downward-sloping trend.\n",
        "    - A scatter plot of negatively correlated variables will show a downward-sloping trend.\n",
        "14. How can you find correlation between variables in Python?\n",
        "    - You can find the correlation between variables in Python using the corr() method of a Pandas DataFrame. This method calculates the pairwise correlation of columns, excluding NA/null values.\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "    - Causation means that one variable directly causes a change in another.\n",
        "    - If X causes Y, then changing X will produce a change in Y.\n",
        "    - Ex: Pushing a ball (X) causes it to move (Y).\n",
        "        This is a direct cause-effect relationship.\n",
        "    - Correlation does NOT imply causation.Just because two things happen together doesn’t mean one causes the other.\n",
        "    - A classic example to illustrate the difference is the correlation between ice cream sales and drowning incidents.\n",
        "         - Correlation: You might observe a strong positive correlation between ice cream sales and the number of drowning incidents during the summer months. As ice cream sales increase, the number of drownings also tends to increase.\n",
        "         - Causation: Does eating ice cream cause people to drown? No. While the two are correlated, there is no direct causal link between them.\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "    - In the context of machine learning, an optimizer is an algorithm or method used to minimize (or maximize) the loss function of a model during training. The goal of an optimizer is to find the set of model parameters (weights and biases) that result in the lowest possible loss, leading to better model performance.\n",
        "    - Here are some common types of optimizers:\n",
        "       - Gradient Descent (GD):The most basic optimizer. It calculates the gradient of the loss function with respect to the model parameters on the entire training dataset. Then, it updates the parameters by taking a step in the opposite direction of the gradient, scaled by the learning rate.\n",
        "       - Stochastic Gradient Descent (SGD):Instead of calculating the gradient on the entire dataset, SGD calculates the gradient and updates parameters using only a single randomly selected data point at a time\n",
        "       - Mini-batch Gradient Descent:A compromise between GD and SGD. It calculates the gradient and updates parameters using a small batch of randomly selected data points (typically between 32 and 256) instead of the entire dataset or a single data point.\n",
        "       - Adam (Adaptive Moment Estimation): An adaptive learning rate optimization algorithm. It calculates an exponential moving average of the gradient and the squared gradient. It then uses these moving averages to adapt the learning rate for each parameter individually.\n",
        "17. What is sklearn.linear_model ?\n",
        "    - sklearn.linear_model is a module in Scikit-learn that provides classes and functions to implement linear models for regression and classification.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given?\n",
        "    - model.fit() is a core method in machine learning used to train the model on your dataset\n",
        "    - Finds the best parameters (like weights and bias) that minimize the loss function.\n",
        "    - Fits the model to the training data (i.e., learns from the data).\n",
        "    - It computes the loss (error between predicted and true values)\n",
        "    - Uses an optimizer (like gradient descent) to adjust weights.\n",
        "    - Stops when loss is minimized or max iterations are reached.\n",
        "\n",
        "    - Can be used to make predictions.\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "    - The model.predict() method is used to generate predictions (outputs) from a trained machine learning or deep learning model. It takes input data and returns the model's predictions based on what it has learned during training.\n",
        "\n",
        "    - Applies the trained model to new input data.\n",
        "    - Returns predictions (e.g., class labels, probabilities, or regression values)\n",
        "    - Works in both batch (multiple samples) and single-sample mode.\n",
        "20. What are continuous and categorical variables?\n",
        "    - In statistics and machine learning, variables (features) can be broadly classified into continuous and categorical types based on their nature and how they represent data.\n",
        "\n",
        "    - continuous variables: Represent measurable quantities that can take any numerical value within a range.Can be finite or infinite (e.g., decimals, fractions).eg:age,temp,income,height etc.\n",
        "       - Can perform arithmetic operations\n",
        "       - Statistical measures: Mean, median, standard deviation.\n",
        "       - Visualized using histograms, scatter plots, line graphs.\n",
        "    - categorial variables : Represent discrete, qualitative groups or labels.\n",
        "      Values fall into distinct categories with no numerical meaning.\n",
        "         -  No arithmetic operations\n",
        "         -  Statistical measures: Mode, frequency tables\n",
        "         -  Visualized using bar charts, pie charts.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "    - Feature scaling is a preprocessing technique used to standardize or normalize the range of independent variables (features) in a dataset. Since features often have different units (e.g., age in years vs. salary in dollars), scaling ensures they contribute equally to model performance.\n",
        "\n",
        "    - 1. Standardization (Z-Score Normalization): Rescales data to have a mean of 0 and standard deviation of 1\n",
        "    - 2. Min-Max Normalization : Scales data to a fixed range (typically [0, 1]).\n",
        "        - Why is Feature Scaling Important?\n",
        "        -  Improves Model Convergence\n",
        "        -  Ensures Fair Feature Contribution\n",
        "        - Essential for Regularization\n",
        "        - Faster convergence during training (e.g., in gradient descent).\n",
        "        - Prevents bias toward features with large numeric ranges.\n",
        "        - Essential for regularization (L1, L2 in Ridge/Lasso).\n",
        "\n",
        "    - Feature scaling ensures fair contribution of each feature and improves model performance and training efficiency—especially for algorithms sensitive to magnitude or distance.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "    - You can easily scale features using Scikit-learn’s preprocessing module.\n",
        "         -  1. Min-Max Scaling : Rescales features to a range between 0 and 1\n",
        "         -  2. Standardization (Z-score Scaling) : Transforms features to have mean = 0 and standard deviation = 1\n",
        "         - 3. Robust Scaling : Uses median and IQR, robust to outliers\n",
        "         - MaxAbs Scaling : Scales each feature by its maximum absolute value to range [-1, 1]\n",
        "         -  Always fit the scaler on training data and transform both training and test data using the same scaler:\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "    - sklearn.preprocessing is a module in Python's Scikit-learn library that provides various utility functions and transformer classes to preprocess and transform raw data into a format that's more suitable for machine learning models.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "    - You can split your data into training and testing sets in Python using the train_test_split function from the sklearn.model_selection module. This is a common and convenient way to prepare your data for machine learning model training and evaluation.\n",
        "25. Explain data encoding?\n",
        "    - Data encoding is the process of converting data from one format to another, typically to make it suitable for use in machine learning algorithms. In the context of machine learning, it most commonly refers to converting categorical variables (variables that represent categories or labels) into a numerical format that machine learning models can understand and process.\n",
        "    - Most machine learning algorithms are designed to work with numerical data. They perform mathematical operations on the input features to make predictions or classifications. String-based categorical variables, like \"color\" (with values \"red\", \"blue\", \"green\") or \"country\" (with values \"USA\", \"Canada\", \"Mexico\"), cannot be directly used in these calculations. Data encoding addresses this by transforming these categories into numerical representations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "       \n",
        "\n"
      ],
      "metadata": {
        "id": "yT1oIg45jwXv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tBY1XdXaxkdP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}